name: example_dag2
schedule_interval: 'rate(10 minutes)'
#active: false

nodes:
  send_table_names:
    function: typhoon.flow_control.branch
    config:
      branches:
        - person
        - job
        - property

  extract_table:
    function: typhoon.relational.execute_query
    config:
      conn_id: test_db
      schema: public

  load_csv_s3:
    function: typhoon.filesystem.write_data
    config:
      conn_id: s3_data_lake


edges:
  send_extraction_params:
    source: send_table_names
    adapter:
      table_name => APPLY: $SOURCE
      query => APPLY:
        - str("SELECT * FROM {{ table_name }} WHERE creation_date='{{ date_string }}'")
        - typhoon.templates.render(template=$1, table_name=$SOURCE, date_string=$DAG_CONFIG.ds)
      batch_size: 2
    destination: extract_table

  table_to_s3:
    async: false        # The table may be large, it doesn't make sense to serialize each batch and send asynchronously
    source: extract_table
    adapter:
      data => APPLY:
        - typhoon.db_result.to_csv(description=$SOURCE.columns, data=$SOURCE.batch)
        - $1.encode('utf_8')
      path => APPLY:
        - str('{{ system_name }}/{{ entity }}/{{ dag_config.ds }}/{{ dag_config.etl_timestamp }}_{{ entity }}_part{{ part_num }}.{{ extension }}')
        - typhoon.templates.render($1, system_name='postgres', entity=$SOURCE.table_name, dag_config=$DAG_CONFIG, part_num=$BATCH_NUM, extension='csv')
    destination: load_csv_s3

